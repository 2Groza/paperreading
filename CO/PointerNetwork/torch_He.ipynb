{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '7'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def to_cuda(x):\n",
    "    if torch.cuda.is_available():\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "class PointerNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, weight_size, is_GRU=False):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.weight_size = weight_size\n",
    "        self.is_GRU = is_GRU\n",
    "\n",
    "        if self.is_GRU:\n",
    "            RNN = nn.GRU\n",
    "            RNNCell = nn.GRUCell\n",
    "        else:\n",
    "            RNN = nn.LSTM\n",
    "            RNNCell = nn.LSTMCell\n",
    "\n",
    "        self.encoder = RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.decoder = RNNCell(input_size, hidden_size)\n",
    "        \n",
    "        self.W1 = nn.Linear(hidden_size, weight_size, bias=False) \n",
    "        self.W2 = nn.Linear(hidden_size, weight_size, bias=False) \n",
    "        self.vt = nn.Linear(weight_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, input):\n",
    "        batch_size = input.shape[0]\n",
    "        decoder_seq_len = input.shape[1]\n",
    "\n",
    "        encoder_output, hc = self.encoder(input) \n",
    "\n",
    "        # Decoding states initialization\n",
    "        hidden = encoder_output[:, -1, :] #hidden state for decoder is the last timestep's output of encoder \n",
    "        if not self.is_GRU: #For LSTM, cell state is the sencond state output\n",
    "            cell = hc[1][-1, :, :]\n",
    "        decoder_input = to_cuda(torch.rand(batch_size, self.input_size))  \n",
    "        \n",
    "        # Decoding with attention             \n",
    "        probs = []\n",
    "        encoder_output = encoder_output.transpose(1, 0) #Transpose the matrix for mm\n",
    "        for i in range(decoder_seq_len):  \n",
    "            if self.is_GRU:\n",
    "                hidden = self.decoder(decoder_input, hidden) \n",
    "            else:\n",
    "                hidden, decoder_hc = self.decoder(decoder_input, (hidden, cell)) \n",
    "            # Compute attention\n",
    "            sum = torch.tanh(self.W1(encoder_output) + self.W2(hidden))    \n",
    "            out = self.vt(sum).squeeze()        \n",
    "            out = F.log_softmax(out.transpose(0, 1).contiguous(), -1)  \n",
    "            probs.append(out)\n",
    "\n",
    "        probs = torch.stack(probs, dim=1)           \n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training... \n",
      "Epoch: 0, Loss: 1.51756\n",
      "Acc: 0.00%\n",
      "Epoch: 2, Loss: 0.76247\n",
      "Acc: 4.28%\n",
      "Epoch: 4, Loss: 0.56312\n",
      "Acc: 13.73%\n",
      "Epoch: 6, Loss: 0.52007\n",
      "Acc: 20.19%\n",
      "Epoch: 8, Loss: 0.38695\n",
      "Acc: 24.34%\n",
      "Epoch: 10, Loss: 0.33419\n",
      "Acc: 33.58%\n",
      "Epoch: 12, Loss: 0.27899\n",
      "Acc: 44.11%\n",
      "Epoch: 14, Loss: 0.25595\n",
      "Acc: 41.47%\n",
      "Epoch: 16, Loss: 0.21022\n",
      "Acc: 50.84%\n",
      "Epoch: 18, Loss: 0.39901\n",
      "Acc: 29.90%\n",
      "Epoch: 20, Loss: 0.28763\n",
      "Acc: 46.69%\n",
      "Epoch: 22, Loss: 0.22793\n",
      "Acc: 47.99%\n",
      "Epoch: 24, Loss: 0.27763\n",
      "Acc: 43.03%\n",
      "Epoch: 26, Loss: 0.17160\n",
      "Acc: 66.91%\n",
      "Epoch: 28, Loss: 0.13706\n",
      "Acc: 70.44%\n",
      "Epoch: 30, Loss: 0.15366\n",
      "Acc: 69.78%\n",
      "Epoch: 32, Loss: 0.11903\n",
      "Acc: 72.91%\n",
      "Epoch: 34, Loss: 0.11094\n",
      "Acc: 76.33%\n",
      "Epoch: 36, Loss: 0.10848\n",
      "Acc: 75.10%\n",
      "Epoch: 38, Loss: 0.69810\n",
      "Acc: 3.19%\n",
      "Epoch: 40, Loss: 0.39384\n",
      "Acc: 10.68%\n",
      "Epoch: 42, Loss: 0.29097\n",
      "Acc: 44.64%\n",
      "Epoch: 44, Loss: 0.34809\n",
      "Acc: 33.40%\n",
      "Epoch: 46, Loss: 0.24415\n",
      "Acc: 50.91%\n",
      "Epoch: 48, Loss: 0.20808\n",
      "Acc: 57.88%\n",
      "Epoch: 50, Loss: 0.17669\n",
      "Acc: 62.22%\n",
      "Epoch: 52, Loss: 0.14714\n",
      "Acc: 66.23%\n",
      "Epoch: 54, Loss: 0.14968\n",
      "Acc: 70.41%\n",
      "Epoch: 56, Loss: 0.13879\n",
      "Acc: 72.69%\n",
      "Epoch: 58, Loss: 0.23248\n",
      "Acc: 16.39%\n",
      "Epoch: 60, Loss: 0.48638\n",
      "Acc: 17.63%\n",
      "Epoch: 62, Loss: 0.27889\n",
      "Acc: 42.12%\n",
      "Epoch: 64, Loss: 0.21325\n",
      "Acc: 54.87%\n",
      "Epoch: 66, Loss: 0.18236\n",
      "Acc: 56.18%\n",
      "Epoch: 68, Loss: 0.15029\n",
      "Acc: 65.99%\n",
      "Epoch: 70, Loss: 0.12144\n",
      "Acc: 70.56%\n",
      "Epoch: 72, Loss: 0.11340\n",
      "Acc: 71.36%\n",
      "Epoch: 74, Loss: 0.10381\n",
      "Acc: 78.24%\n",
      "Epoch: 76, Loss: 0.09115\n",
      "Acc: 76.53%\n",
      "Epoch: 78, Loss: 0.08758\n",
      "Acc: 80.04%\n",
      "Epoch: 80, Loss: 0.07597\n",
      "Acc: 84.34%\n",
      "Epoch: 82, Loss: 0.08437\n",
      "Acc: 82.04%\n",
      "Epoch: 84, Loss: 0.08542\n",
      "Acc: 71.11%\n",
      "Epoch: 86, Loss: 0.04875\n",
      "Acc: 83.84%\n",
      "Epoch: 88, Loss: 0.05760\n",
      "Acc: 86.78%\n",
      "Epoch: 90, Loss: 0.05702\n",
      "Acc: 89.49%\n",
      "Epoch: 92, Loss: 0.07387\n",
      "Acc: 86.78%\n",
      "Epoch: 94, Loss: 0.05349\n",
      "Acc: 89.21%\n",
      "Epoch: 96, Loss: 0.05470\n",
      "Acc: 88.66%\n",
      "Epoch: 98, Loss: 0.86298\n",
      "Acc: 0.91%\n",
      "Epoch: 100, Loss: 0.31629\n",
      "Acc: 36.38%\n",
      "Epoch: 102, Loss: 0.19704\n",
      "Acc: 60.34%\n",
      "Epoch: 104, Loss: 0.16277\n",
      "Acc: 66.54%\n",
      "Epoch: 106, Loss: 0.14009\n",
      "Acc: 72.51%\n",
      "Epoch: 108, Loss: 0.10300\n",
      "Acc: 77.86%\n",
      "Epoch: 110, Loss: 0.09244\n",
      "Acc: 80.46%\n",
      "Epoch: 112, Loss: 0.13510\n",
      "Acc: 62.56%\n",
      "Epoch: 114, Loss: 0.06433\n",
      "Acc: 84.23%\n",
      "Epoch: 116, Loss: 0.08593\n",
      "Acc: 83.11%\n",
      "Epoch: 118, Loss: 0.06989\n",
      "Acc: 87.11%\n",
      "Epoch: 120, Loss: 0.05435\n",
      "Acc: 87.68%\n",
      "Epoch: 122, Loss: 0.06712\n",
      "Acc: 86.32%\n",
      "Epoch: 124, Loss: 0.04867\n",
      "Acc: 89.91%\n",
      "Epoch: 126, Loss: 0.04309\n",
      "Acc: 91.16%\n",
      "Epoch: 128, Loss: 0.05158\n",
      "Acc: 91.71%\n",
      "Epoch: 130, Loss: 0.87644\n",
      "Acc: 0.79%\n",
      "Epoch: 132, Loss: 0.31297\n",
      "Acc: 37.90%\n",
      "Epoch: 134, Loss: 0.21393\n",
      "Acc: 58.70%\n",
      "Epoch: 136, Loss: 0.15252\n",
      "Acc: 67.77%\n",
      "Epoch: 138, Loss: 0.12930\n",
      "Acc: 75.97%\n",
      "Epoch: 140, Loss: 0.11044\n",
      "Acc: 77.91%\n",
      "Epoch: 142, Loss: 0.09009\n",
      "Acc: 80.67%\n",
      "Epoch: 144, Loss: 0.08689\n",
      "Acc: 83.20%\n",
      "Epoch: 146, Loss: 0.08158\n",
      "Acc: 74.28%\n",
      "Epoch: 148, Loss: 0.06876\n",
      "Acc: 84.98%\n",
      "Epoch: 150, Loss: 0.06932\n",
      "Acc: 83.42%\n",
      "Epoch: 152, Loss: 0.12473\n",
      "Acc: 76.98%\n",
      "Epoch: 154, Loss: 0.06111\n",
      "Acc: 88.89%\n",
      "Epoch: 156, Loss: 0.04111\n",
      "Acc: 91.58%\n",
      "Epoch: 158, Loss: 0.04123\n",
      "Acc: 91.58%\n",
      "Epoch: 160, Loss: 0.04790\n",
      "Acc: 90.77%\n",
      "Epoch: 162, Loss: 0.03893\n",
      "Acc: 91.48%\n",
      "Epoch: 164, Loss: 0.04351\n",
      "Acc: 91.27%\n",
      "Epoch: 166, Loss: 0.03950\n",
      "Acc: 88.87%\n",
      "Epoch: 168, Loss: 0.48317\n",
      "Acc: 15.23%\n",
      "Epoch: 170, Loss: 0.23395\n",
      "Acc: 52.92%\n",
      "Epoch: 172, Loss: 0.16012\n",
      "Acc: 68.19%\n",
      "Epoch: 174, Loss: 0.11963\n",
      "Acc: 75.92%\n",
      "Epoch: 176, Loss: 0.10404\n",
      "Acc: 80.58%\n",
      "Epoch: 178, Loss: 0.08805\n",
      "Acc: 83.24%\n",
      "Epoch: 180, Loss: 0.08127\n",
      "Acc: 86.58%\n",
      "Epoch: 182, Loss: 0.06770\n",
      "Acc: 88.11%\n",
      "Epoch: 184, Loss: 0.05668\n",
      "Acc: 89.56%\n",
      "Epoch: 186, Loss: 0.06073\n",
      "Acc: 90.89%\n",
      "Epoch: 188, Loss: 0.04322\n",
      "Acc: 92.86%\n",
      "Epoch: 190, Loss: 0.05308\n",
      "Acc: 91.38%\n",
      "Epoch: 192, Loss: 0.04221\n",
      "Acc: 93.07%\n",
      "Epoch: 194, Loss: 0.05204\n",
      "Acc: 89.41%\n",
      "Epoch: 196, Loss: 0.03568\n",
      "Acc: 94.88%\n",
      "Epoch: 198, Loss: 0.03931\n",
      "Acc: 95.19%\n",
      "Epoch: 200, Loss: 0.02775\n",
      "Acc: 96.59%\n",
      "Epoch: 202, Loss: 0.02562\n",
      "Acc: 93.86%\n",
      "Epoch: 204, Loss: 0.02412\n",
      "Acc: 96.20%\n",
      "Epoch: 206, Loss: 0.02574\n",
      "Acc: 95.23%\n",
      "Epoch: 208, Loss: 0.02621\n",
      "Acc: 94.63%\n",
      "Epoch: 210, Loss: 0.02081\n",
      "Acc: 97.56%\n",
      "Epoch: 212, Loss: 0.01691\n",
      "Acc: 98.20%\n",
      "Epoch: 214, Loss: 0.02244\n",
      "Acc: 98.21%\n",
      "Epoch: 216, Loss: 0.01877\n",
      "Acc: 97.29%\n",
      "Epoch: 218, Loss: 0.01844\n",
      "Acc: 98.39%\n",
      "Epoch: 220, Loss: 0.44538\n",
      "Acc: 21.78%\n",
      "Epoch: 222, Loss: 0.17724\n",
      "Acc: 65.52%\n",
      "Epoch: 224, Loss: 0.13192\n",
      "Acc: 77.34%\n",
      "Epoch: 226, Loss: 0.08940\n",
      "Acc: 82.18%\n",
      "Epoch: 228, Loss: 0.07388\n",
      "Acc: 87.51%\n",
      "Epoch: 230, Loss: 0.07018\n",
      "Acc: 88.21%\n",
      "Epoch: 232, Loss: 0.05618\n",
      "Acc: 90.50%\n",
      "Epoch: 234, Loss: 0.04836\n",
      "Acc: 91.80%\n",
      "Epoch: 236, Loss: 0.04733\n",
      "Acc: 92.43%\n",
      "Epoch: 238, Loss: 0.04257\n",
      "Acc: 94.11%\n",
      "Epoch: 240, Loss: 0.03506\n",
      "Acc: 95.06%\n",
      "Epoch: 242, Loss: 0.03728\n",
      "Acc: 96.41%\n",
      "Epoch: 244, Loss: 0.03109\n",
      "Acc: 96.63%\n",
      "Epoch: 246, Loss: 0.03320\n",
      "Acc: 95.39%\n",
      "Epoch: 248, Loss: 0.02577\n",
      "Acc: 96.91%\n",
      "Epoch: 250, Loss: 0.02017\n",
      "Acc: 98.10%\n",
      "Epoch: 252, Loss: 0.01864\n",
      "Acc: 98.32%\n",
      "Epoch: 254, Loss: 0.01529\n",
      "Acc: 98.74%\n",
      "Epoch: 256, Loss: 0.05602\n",
      "Acc: 90.97%\n",
      "Epoch: 258, Loss: 0.01861\n",
      "Acc: 97.79%\n",
      "Epoch: 260, Loss: 0.01670\n",
      "Acc: 99.37%\n",
      "Epoch: 262, Loss: 0.01186\n",
      "Acc: 99.37%\n",
      "Epoch: 264, Loss: 0.01465\n",
      "Acc: 99.50%\n",
      "Epoch: 266, Loss: 0.01380\n",
      "Acc: 98.94%\n",
      "Epoch: 268, Loss: 0.01504\n",
      "Acc: 97.80%\n",
      "Epoch: 270, Loss: 0.01329\n",
      "Acc: 97.16%\n",
      "Epoch: 272, Loss: 0.02123\n",
      "Acc: 98.04%\n",
      "Epoch: 274, Loss: 0.00824\n",
      "Acc: 99.67%\n",
      "Epoch: 276, Loss: 0.00607\n",
      "Acc: 99.81%\n",
      "Epoch: 278, Loss: 0.00593\n",
      "Acc: 99.69%\n",
      "Epoch: 280, Loss: 0.00742\n",
      "Acc: 99.93%\n",
      "Epoch: 282, Loss: 0.00446\n",
      "Acc: 99.70%\n",
      "Epoch: 284, Loss: 0.11416\n",
      "Acc: 68.67%\n",
      "Epoch: 286, Loss: 0.02357\n",
      "Acc: 97.46%\n",
      "Epoch: 288, Loss: 0.01022\n",
      "Acc: 99.27%\n",
      "Epoch: 290, Loss: 0.00700\n",
      "Acc: 99.77%\n",
      "Epoch: 292, Loss: 0.00563\n",
      "Acc: 99.93%\n",
      "Epoch: 294, Loss: 0.00638\n",
      "Acc: 99.96%\n",
      "Epoch: 296, Loss: 0.00463\n",
      "Acc: 99.99%\n",
      "Epoch: 298, Loss: 0.00458\n",
      "Acc: 99.96%\n",
      "Epoch: 300, Loss: 0.00437\n",
      "Acc: 99.99%\n",
      "Epoch: 302, Loss: 0.00383\n",
      "Acc: 99.99%\n",
      "Epoch: 304, Loss: 0.00261\n",
      "Acc: 100.00%\n",
      "Epoch: 306, Loss: 0.00316\n",
      "Acc: 99.99%\n",
      "Epoch: 308, Loss: 0.00473\n",
      "Acc: 99.71%\n",
      "Epoch: 310, Loss: 0.17141\n",
      "Acc: 65.11%\n",
      "Epoch: 312, Loss: 0.06740\n",
      "Acc: 83.62%\n",
      "Epoch: 314, Loss: 0.04112\n",
      "Acc: 93.27%\n",
      "Epoch: 316, Loss: 0.03521\n",
      "Acc: 96.14%\n",
      "Epoch: 318, Loss: 0.02900\n",
      "Acc: 97.43%\n",
      "Epoch: 320, Loss: 0.02325\n",
      "Acc: 97.77%\n",
      "Epoch: 322, Loss: 0.02238\n",
      "Acc: 98.81%\n",
      "Epoch: 324, Loss: 0.01690\n",
      "Acc: 98.59%\n",
      "Epoch: 326, Loss: 0.01414\n",
      "Acc: 99.61%\n",
      "Epoch: 328, Loss: 0.01444\n",
      "Acc: 99.72%\n",
      "Epoch: 330, Loss: 0.01666\n",
      "Acc: 96.93%\n",
      "Epoch: 332, Loss: 0.00903\n",
      "Acc: 99.86%\n",
      "Epoch: 334, Loss: 0.00852\n",
      "Acc: 99.92%\n",
      "Epoch: 336, Loss: 0.00828\n",
      "Acc: 99.93%\n",
      "Epoch: 338, Loss: 0.00688\n",
      "Acc: 99.90%\n",
      "Epoch: 340, Loss: 0.00627\n",
      "Acc: 99.96%\n",
      "Epoch: 342, Loss: 0.00651\n",
      "Acc: 99.99%\n",
      "Epoch: 344, Loss: 0.00544\n",
      "Acc: 99.98%\n",
      "Epoch: 346, Loss: 0.00488\n",
      "Acc: 100.00%\n",
      "Epoch: 348, Loss: 0.00361\n",
      "Acc: 100.00%\n",
      "Epoch: 350, Loss: 0.00459\n",
      "Acc: 99.99%\n",
      "Epoch: 352, Loss: 0.00342\n",
      "Acc: 100.00%\n",
      "Epoch: 354, Loss: 0.00371\n",
      "Acc: 100.00%\n",
      "Epoch: 356, Loss: 0.00307\n",
      "Acc: 100.00%\n",
      "Epoch: 358, Loss: 0.00323\n",
      "Acc: 100.00%\n",
      "Epoch: 360, Loss: 0.00342\n",
      "Acc: 100.00%\n",
      "Epoch: 362, Loss: 0.00290\n",
      "Acc: 100.00%\n",
      "Epoch: 364, Loss: 0.07941\n",
      "Acc: 78.73%\n",
      "Epoch: 366, Loss: 0.18463\n",
      "Acc: 62.89%\n",
      "Epoch: 368, Loss: 0.02528\n",
      "Acc: 97.39%\n",
      "Epoch: 370, Loss: 0.01286\n",
      "Acc: 99.74%\n",
      "Epoch: 372, Loss: 0.00773\n",
      "Acc: 99.99%\n",
      "Epoch: 374, Loss: 0.00670\n",
      "Acc: 100.00%\n",
      "Epoch: 376, Loss: 0.00639\n",
      "Acc: 100.00%\n",
      "Epoch: 378, Loss: 0.00518\n",
      "Acc: 100.00%\n",
      "Epoch: 380, Loss: 0.00475\n",
      "Acc: 100.00%\n",
      "Epoch: 382, Loss: 0.00349\n",
      "Acc: 100.00%\n",
      "Epoch: 384, Loss: 0.00378\n",
      "Acc: 100.00%\n",
      "Epoch: 386, Loss: 0.00377\n",
      "Acc: 100.00%\n",
      "Epoch: 388, Loss: 0.00273\n",
      "Acc: 100.00%\n",
      "Epoch: 390, Loss: 0.00241\n",
      "Acc: 100.00%\n",
      "Epoch: 392, Loss: 0.00282\n",
      "Acc: 100.00%\n",
      "Epoch: 394, Loss: 0.00212\n",
      "Acc: 100.00%\n",
      "Epoch: 396, Loss: 0.00177\n",
      "Acc: 100.00%\n",
      "Epoch: 398, Loss: 0.00217\n",
      "Acc: 100.00%\n",
      "Epoch: 400, Loss: 0.00187\n",
      "Acc: 100.00%\n",
      "Epoch: 402, Loss: 0.00167\n",
      "Acc: 100.00%\n",
      "Epoch: 404, Loss: 0.00186\n",
      "Acc: 100.00%\n",
      "Epoch: 406, Loss: 0.00243\n",
      "Acc: 100.00%\n",
      "Epoch: 408, Loss: 0.07393\n",
      "Acc: 84.34%\n",
      "Epoch: 410, Loss: 0.01063\n",
      "Acc: 98.41%\n",
      "Epoch: 412, Loss: 0.00846\n",
      "Acc: 97.11%\n",
      "Epoch: 414, Loss: 0.00830\n",
      "Acc: 99.34%\n",
      "Epoch: 416, Loss: 0.00347\n",
      "Acc: 99.82%\n",
      "Epoch: 418, Loss: 0.00180\n",
      "Acc: 100.00%\n",
      "Epoch: 420, Loss: 0.00106\n",
      "Acc: 100.00%\n",
      "Epoch: 422, Loss: 0.00118\n",
      "Acc: 100.00%\n",
      "Epoch: 424, Loss: 0.00099\n",
      "Acc: 100.00%\n",
      "Epoch: 426, Loss: 0.00085\n",
      "Acc: 100.00%\n",
      "Epoch: 428, Loss: 0.00075\n",
      "Acc: 100.00%\n",
      "Epoch: 430, Loss: 0.00088\n",
      "Acc: 100.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 432, Loss: 0.00076\n",
      "Acc: 100.00%\n",
      "Epoch: 434, Loss: 0.00072\n",
      "Acc: 100.00%\n",
      "Epoch: 436, Loss: 0.00073\n",
      "Acc: 100.00%\n",
      "Epoch: 438, Loss: 0.00060\n",
      "Acc: 100.00%\n",
      "Epoch: 440, Loss: 0.00056\n",
      "Acc: 100.00%\n",
      "Epoch: 442, Loss: 0.00047\n",
      "Acc: 100.00%\n",
      "Epoch: 444, Loss: 0.00055\n",
      "Acc: 100.00%\n",
      "Epoch: 446, Loss: 0.00044\n",
      "Acc: 100.00%\n",
      "Epoch: 448, Loss: 0.00047\n",
      "Acc: 100.00%\n",
      "Epoch: 450, Loss: 0.00043\n",
      "Acc: 100.00%\n",
      "Epoch: 452, Loss: 0.00047\n",
      "Acc: 100.00%\n",
      "Epoch: 454, Loss: 0.00040\n",
      "Acc: 100.00%\n",
      "Epoch: 456, Loss: 0.00044\n",
      "Acc: 100.00%\n",
      "Epoch: 458, Loss: 0.00039\n",
      "Acc: 100.00%\n",
      "Epoch: 460, Loss: 0.00041\n",
      "Acc: 100.00%\n",
      "Epoch: 462, Loss: 0.00032\n",
      "Acc: 100.00%\n",
      "Epoch: 464, Loss: 0.00031\n",
      "Acc: 100.00%\n",
      "Epoch: 466, Loss: 0.00024\n",
      "Acc: 100.00%\n",
      "Epoch: 468, Loss: 0.00025\n",
      "Acc: 100.00%\n",
      "Epoch: 470, Loss: 0.00023\n",
      "Acc: 100.00%\n",
      "Epoch: 472, Loss: 0.00029\n",
      "Acc: 100.00%\n",
      "Epoch: 474, Loss: 0.00025\n",
      "Acc: 100.00%\n",
      "Epoch: 476, Loss: 0.00023\n",
      "Acc: 100.00%\n",
      "Epoch: 478, Loss: 0.00023\n",
      "Acc: 100.00%\n",
      "Epoch: 480, Loss: 0.00020\n",
      "Acc: 100.00%\n",
      "Epoch: 482, Loss: 0.00019\n",
      "Acc: 100.00%\n",
      "Epoch: 484, Loss: 0.00016\n",
      "Acc: 100.00%\n",
      "Epoch: 486, Loss: 1.13764\n",
      "Acc: 1.78%\n",
      "Epoch: 488, Loss: 0.16084\n",
      "Acc: 64.12%\n",
      "Epoch: 490, Loss: 0.08993\n",
      "Acc: 80.21%\n",
      "Epoch: 492, Loss: 0.06579\n",
      "Acc: 86.93%\n",
      "Epoch: 494, Loss: 0.07056\n",
      "Acc: 90.88%\n",
      "Epoch: 496, Loss: 0.04544\n",
      "Acc: 93.82%\n",
      "Epoch: 498, Loss: 0.03226\n",
      "Acc: 95.41%\n",
      "Test...\n",
      "Acc: 83.20%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "#from model import PointerNetwork\n",
    "\n",
    "\n",
    "EPOCH = 500\n",
    "BATCH_SIZE = 250\n",
    "DATA_SIZE = 10000\n",
    "INPUT_SIZE = 1\n",
    "HIDDEN_SIZE = 512\n",
    "WEIGHT_SIZE = 256\n",
    "LR = 0.001\n",
    "\n",
    "\n",
    "def to_cuda(x):\n",
    "    if torch.cuda.is_available():\n",
    "        return x.cuda()\n",
    "    return x\n",
    "\n",
    "def getdata(experiment, data_size):\n",
    "    if experiment == 1:\n",
    "        high = 100\n",
    "        senlen = 5\n",
    "        x = np.array([np.random.choice(range(high), senlen, replace=False)\n",
    "                      for _ in range(data_size)])\n",
    "        y = np.argsort(x)\n",
    "    elif experiment == 2:\n",
    "        high = 100\n",
    "        senlen = 10\n",
    "        x = np.array([np.random.choice(range(high), senlen, replace=False)\n",
    "                      for _ in range(data_size)])\n",
    "        y = np.argsort(x)\n",
    "    elif experiment == 3:\n",
    "        senlen = 5\n",
    "        x = np.array([np.random.random(senlen) for _ in range(data_size)])\n",
    "        y = np.argsort(x)\n",
    "    elif experiment == 4:\n",
    "        senlen = 10\n",
    "        x = np.array([np.random.random(senlen) for _ in range(data_size)])\n",
    "        y = np.argsort(x)\n",
    "    return x, y\n",
    "\n",
    "def evaluate(model, X, Y):\n",
    "    probs = model(X) \n",
    "    prob, indices = torch.max(probs, 2) \n",
    "    equal_cnt = sum([1 if torch.equal(index.detach(), y.detach()) else 0 for index, y in zip(indices, Y)])\n",
    "    accuracy = equal_cnt/len(X)\n",
    "    print('Acc: {:.2f}%'.format(accuracy*100))\n",
    "\n",
    "#Get Dataset\n",
    "x, y = getdata(experiment=2, data_size = DATA_SIZE)\n",
    "x = to_cuda(torch.FloatTensor(x).unsqueeze(2))     \n",
    "y = to_cuda(torch.LongTensor(y)) \n",
    "#Split Dataset\n",
    "train_size = (int)(DATA_SIZE * 0.9)\n",
    "train_X = x[:train_size]\n",
    "train_Y = y[:train_size]\n",
    "test_X = x[train_size:]\n",
    "test_Y = y[train_size:]\n",
    "#Build DataLoader\n",
    "train_data = Data.TensorDataset(train_X, train_Y)\n",
    "data_loader = Data.DataLoader(\n",
    "    dataset = train_data,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    ")\n",
    "\n",
    "\n",
    "#Define the Model\n",
    "model = PointerNetwork(INPUT_SIZE, HIDDEN_SIZE, WEIGHT_SIZE, is_GRU=False)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "loss_fun = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "#Training...\n",
    "print('Training... ')\n",
    "for epoch in range(EPOCH):\n",
    "    for (batch_x, batch_y) in data_loader:\n",
    "        probs = model(batch_x)         \n",
    "        outputs = probs.view(-1, batch_x.shape[1])\n",
    "        batch_y = batch_y.view(-1) \n",
    "        loss = loss_fun(outputs, batch_y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        print('Epoch: {}, Loss: {:.5f}'.format(epoch, loss.item()))\n",
    "        evaluate(model, train_X, train_Y)\n",
    "#Test...    \n",
    "print('Test...')\n",
    "evaluate(model, test_X, test_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9000, 10, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9000, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8, 9, 2, 3, 1, 4, 6, 7, 0, 5], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[84.],\n",
       "        [65.],\n",
       "        [36.],\n",
       "        [51.],\n",
       "        [74.],\n",
       "        [95.],\n",
       "        [76.],\n",
       "        [77.],\n",
       "        [ 0.],\n",
       "        [23.]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
